---
title: "Interim Assessment - Frequency Model"
author: "Mary Sin Fai Lam (20696951)"
date: "22/02/2022"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

# README
This is a project that I worked on in one of my fourth year courses at the University of Waterloo. The aim was to use insurance claims data to predict claims frequency. In other words, the number of times that a customer is going to have an accident and submit a claim in a particular year. Since these accidents are fairly rare (hopefully!), we have an unbalanced data set where the claim count (response variable) is most of the time 0. 

I used a Quasi-Poisson regression model for this project. The Poisson distribution is useful to model count data which is what we want. However, I used a Quasi-Poisson regression model which is a generalization of the Poisson regression and is used when modeling an overdispersed count variable. The Poisson model assumes that the variance is equal to the mean, which is not a fair assumption for this project.

For feature selection, I've added features one at a time and compared the models' performance using an ANOVA F test, a significance test of the feature and metrics such as AIC/BIC.
As a final validation check, I've compared the Gini coefficient on the holdout set of my top models.

Note: You'll notice that I refer to exposures a lot throughout the script, exposures are essentially how long a customer has been insured by company in a particular year. E.g A customer who has been insured for half a year will have an exposure of 0.5.

```{r}
library(dplyr)
library(knitr)
setwd("C:/Users/maryj/Google Drive/School/4B/ACTSC 489/Frequency Model")
compData <- read.csv("compDatasetTrain1.csv", header = TRUE)
set.seed(12345)
compData$rand_sample <- runif(nrow(compData), 0, 1)
compData$partition <-ifelse(compData$rand_sample<0.7, "Training", "Holdout")
```

# EDA on critical modelling variables

# EDA on response
```{r}
claim_count_summary = compData %>%
  group_by(Comp_claim_count) %>%
  summarise(record_count = n())
claim_count_summary
```
The response data seem reasonable, with the majority risks having no claim count given that auto claims are relatively rare. There are no negative or non-integer values, which is expected for a claim count variable. There is a small number risks having more than 1 claim and no extremely large values, and there is enough non-zero responses to build a reasonable model.

# EDA on exposure
Let's check the minimum, median and maximum for the exposures:
```{r}
compData %>% 
  pull(Comp_earned_count) %>%
  quantile(probs = c(0, 0.5, 1))
```
The exposures also seem reasonable with the lowest value being close to 0 (but not 0) and the highest value not exceeding 1 (as expected in 1 policy year). The overall distribution looks reasonable.

Checking the frequency distribution across different accident years, we see that there are some discrepancies in frequency, most likely attributed to trend over the years. We'll account for that by introducing a temporal control variable in our models.

```{r}
compData %>%
  group_by(Accident_year) %>%
  summarise(exposure_total = sum(Comp_earned_count),
            numclaims_total = sum(Comp_claim_count)) %>%
  mutate(frequency = numclaims_total / exposure_total)
```
Now, we start the modelling process by dividing our data into training and holdout set using a 70-30 split.
```{r}
overall_exposure = compData %>%
  pull(Comp_earned_count) %>%
  sum()
compData %>%
  group_by(partition) %>%
  summarise(number_of_records = n(),
            exposure_total = sum(Comp_earned_count),
            exposure_pct = sum(Comp_earned_count) / overall_exposure,
            numclaims_total = sum(Comp_claim_count)) %>%
  mutate(frequency = numclaims_total / exposure_total)
```
Following some validation, we can see that there is a 70-30 split of exposure between training and holdout and the frequency for both are similar, so there is no evidence of bias between the two.

```{r}
# Frequency GLM - Initial Model Form
GLM_initial <- glm(Comp_claim_count ~ factor(Accident_year), 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
```

# Fitting predictors
# Marital Status
We start with GLM_initial, a glm fit with just the temporal control variable and our log(exposure) offset. Now, we want to add Marital status to our model, we'll call it GLM1. We saw in Assignment 1 that there were many levels to Marital status that were not significant, and there were some missing values. We assign a new level: "Missing" to the missing values. Our exploratory data analysis that indicated that, aside from the “Married” and “Single” levels, most
levels do not have a large amount of exposure. We should group based on the meaning of the levels, we assign "Married" to the levels Married, Same Sex, and
Common Law, and "Single" otherwise.

```{r}
compData = compData %>%
  mutate(Marital_status = case_when(
    Marital_status == "Single" ~ "Single",
    Marital_status == "Widowed" ~ "Single",
    Marital_status == "Divorced" ~ "Single",
    Marital_status == "Separated" ~ "Single",
    Marital_status == "Common Law" ~ "Married",
    Marital_status == "Same sex" ~ "Married",
    Marital_status == "Married" ~ "Married",
    Marital_status == "" ~ "Missing"
))
```

```{r}
compData %>%
  filter(partition == "Training") %>%
  group_by(Marital_status) %>%
  summarise(earned_exposure = sum(Comp_earned_count),
            frequency = sum(Comp_claim_count) / sum(Comp_earned_count))
```

```{r}
GLM1 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM1)
```
We now perform a series of statistical tests, the results are summarised in the following table. Assessing **significance**, the p value is small for "Single". It also seems **reasonable** to do so since it's common in the insurance industry to use marital status as a rating variable. This is mostly because single drivers are usually younger and less experienced, thus are more likely to incur a claim. You can see claim frequency being higher for single drivers compared to their married counterpart in the appendix. For a **parsimony** check, AIC for GLM_initial was 15503 and decreases to 15493 for GLM1 and BIC increases from 15532 to 15541. The model passes AIC but fails BIC test. Performing a **time consistency** test, we fit an interaction between Marital_status and accident year in the
model. The overall effect of Single and Missing levels, for each accident year, is obtained by aggregating the corresponding coefficients from this model. We find that "Single" is consistent across the the three years but "Missing" is not. However, this is not surprising due to low data volume in the "Missing" level. We are more concerned that the "Single" level passes the time consistency test since it has the most number of exposures anyway. Lastly, we perform an F test comparing GLM_initial with our nested model GLM1. The p value is 0.003797 and therefore significant. Following these tests, we conclude that adding Marital_status to our model increases its predictive power, and GLM1 becomes our best model so far.

```{r}
AIC_BIC <- function(model_ODP) {
model_poi <- glm(model_ODP$y~model_ODP$x, family=poisson(link="log"), data=model_ODP$data, offset=model_ODP$offset, subset=model_ODP$subset, na.action="na.pass", x=TRUE)
AIC <- AIC(model_poi)
BIC <- BIC(model_poi)
list("AIC"=AIC, "BIC"=BIC)
}
```

```{r}
AIC_BIC(GLM_initial)
AIC_BIC(GLM1)
anova(GLM_initial,GLM1,test = "F")
```

```{r}
# time consistency
compData = compData %>%
  mutate(AY_factor = as.factor(Accident_year))
glm_marital_status_TC <- glm(Comp_claim_count ~ AY_factor * Marital_status, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(glm_marital_status_TC)
```

```{r}

effect_2018_M = glm_marital_status_TC$coefficients['Marital_statusMissing'] 
effect_2019_M = glm_marital_status_TC$coefficients['Marital_statusMissing'] + glm_marital_status_TC$coefficients['AY_factor2019:Marital_statusMissing']
effect_2020_M = glm_marital_status_TC$coefficients['Marital_statusMissing'] + glm_marital_status_TC$coefficients['AY_factor2020:Marital_statusMissing']

effect_2018 = glm_marital_status_TC$coefficients['Marital_statusSingle']
effect_2019 = glm_marital_status_TC$coefficients['Marital_statusSingle'] + glm_marital_status_TC$coefficients['AY_factor2019:Marital_statusSingle']
effect_2020 = glm_marital_status_TC$coefficients['Marital_statusSingle'] + glm_marital_status_TC$coefficients['AY_factor2020:Marital_statusSingle']

kable(data.frame(c(GLM1$coefficients['Marital_statusMissing'],GLM1$coefficients['Marital_statusSingle']),c(0.140,0.00157),c(effect_2018_M, effect_2018),c(effect_2019_M, effect_2019),c(effect_2020_M, effect_2020)), col.names=c("Coefficients", "P-value", "2018 Effect", "2019 Effect","2020 Effect"), digits = 3)
```

# Insured age
We now wish to create a new variable which we suspect is a good predictor. We create the insured_age variable, which is the how old the insured is. We do so by subtracting the insured birth year from the year of the term effective date. Once created, we find that insured_age has missing values, however this is only for a small number of records and not accounting for a lot of exposures (see appendix for more details). Therefore, we decided to impute the missing values with the median insured age of 48. We also calculated the minimum and maximum value to validate our data:

```{r}
# Creating insured_age variable
compData = compData %>%
  mutate(insured_age = as.numeric(substr(Term_effective_date,0,4)) - as.numeric(substr(Insured_birth_date,0,4)))
```

```{r}
compData %>% 
  filter(partition == "Training") %>%
  mutate(insured_age_missing = is.na(insured_age)) %>%
  group_by(insured_age_missing) %>%
  summarise(number_of_records = n(),
            exposure = sum(Comp_earned_count),
            frequency = sum(Comp_claim_count) / sum(exposure))
```

```{r}
compData %>%
  filter(partition == "Training") %>%
  pull(insured_age) %>%
  quantile(probs = c(0, 0.5, 1), na.rm = TRUE)
```
We can see that the minimum and maximum age seem reasonable, there aren’t any negative or extremely low/high values, although 99 seems a little old to be still driving! But let’s not make any additional modifications and visualize the data, we can see from the graph (in the appendix) that density for much older drivers is small, and density is concentrated at the median. Therefore, let’s add this variable to our GLM1, which we now call GLM2.

```{r}
# Replace missing values with median age of 48
compData = compData %>% 
  mutate(insured_age_missing = is.na(insured_age),
         insured_age_imputed = ifelse(insured_age_missing, 48, insured_age))
```

```{r}
library(ggplot2)
ggplot(data = compData %>% filter(partition == "Training"), aes(x = insured_age_imputed)) + 
  geom_density() +
  geom_vline(xintercept = 48, colour = "red")
```

```{r}
GLM2 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + insured_age_imputed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM2)
```

Similar to before, we perform our statistical tests, the results are summarised in the following table. Assessing **significance**, the p value is very small at 0.000127 for insured_age. From a **reasonability** standpoint, using age as a rating variable is reasonable and common in insurance, since usually younger drivers are less experienced, and thus more likely to incur a claim. You can see from the table below that the coefficient for insured_age is negative suggesting that claim frequency decreases as age increases, which is as expected. For the **parsimony** check, AIC for GLM1 was 15493 and decreases to 15476 for GLM2 and BIC also decreases from 15541 to 15534. Hence, the model passes both AIC and BIC test. Now, let's perform a **time consistency** test, we fit an interaction between insured_age and accident year in the model. The overall effect of insured age, for each accident year, is obtained by aggregating the corresponding coefficients from this model. The results show that insured_age passes the time consistency check. Lastly, we perform an F test comparing GLM1 with GLM2. The p value is 0.0001133 and therefore very significant. Since insured_age passes all of our statistical tests, we keep it in our model, and GLM2 becomes our best model so far.

```{r}
# time consistency
compData = compData %>%
  mutate(AY_factor = as.factor(Accident_year))
glm_insured_age_TC <- glm(Comp_claim_count ~ AY_factor * insured_age_imputed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(glm_insured_age_TC)
```

```{r}

effect_2018 = glm_insured_age_TC$coefficients['insured_age_imputed']
effect_2019 = glm_insured_age_TC$coefficients['insured_age_imputed'] + glm_insured_age_TC$coefficients['AY_factor2019:insured_age_imputed']
effect_2020 = glm_insured_age_TC$coefficients['insured_age_imputed'] + glm_insured_age_TC$coefficients['AY_factor2020:insured_age_imputed']

kable(data.frame(GLM2$coefficients['insured_age_imputed'],0.000127,effect_2018,effect_2019,effect_2020), col.names=c("Coefficients", "P-value", "2018 Effect", "2019 Effect","2020 Effect"), digits = 6)
```

```{r}
AIC_BIC(GLM1)
AIC_BIC(GLM2)
anova(GLM1,GLM2, test="F")
```
# Vehicle age

We now wish to create the vehicle_age variable, which is simply how old the vehicle is. We do so by subtracting the vehicle model year from the year of the term effective date. Once created, we find that vehicle_age is complete and has no missing values, so we do not modify the data further. Let's validate our data by calculating the minimum, median and maximum value:

```{r}
# Creating vehicle_age variable (there is no missing data)
compData = compData %>%
  mutate(vehicle_age = as.numeric(substr(Term_effective_date,0,4)) - Vehicle_model_year)
```

```{r}
compData %>%
  filter(partition == "Training") %>%
  pull(vehicle_age) %>%
  quantile(probs = c(0,0.5,1), na.rm = TRUE)
```
The newest vehicle is two years old and the oldest 65 years old. The median is around 10. These numbers seem reasonable for vehicle age as there are no negative integers and no extreme high values. The distribution of the variable also seem reasonable, with a positively skewed distribution (see appendix). 

```{r}
ggplot(data = compData %>% filter(partition == "Training"), aes(x = vehicle_age)) + 
  geom_density() +
  geom_vline(xintercept = 10, colour = "red")
```
We fit a new glm model (GLM3) with vehicle_age and the predictors from GLM2.
```{r}
GLM3 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + insured_age_imputed + vehicle_age, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM3)
```
Let's test our new model. The p value is extremely small (~0) for vehicle_age there it is very **significant**. From a **reasonability** standpoint, using vehicle age as a rating variable is reasonable and often used in insurance, we find that the coefficient is negative suggesting that the newer the vehicle, the more likely it is to incur a claim. This makes sense for comprehensive coverage, especially in the case of thefts, as newer vehicles are usually targeted. For the **parsimony** check, both AIC and BIC decreases, from 15476 to 15399 and 15534 to 15466, respectively. Hence, the model passes both AIC and BIC test. Now, let's perform a **time consistency** test, this is done in the same way described above. The results show that vehicle_age is consistent across the three years. Lastly, we perform an F test comparing GLM2 with GLM3. The p value is almost 0 (8.863e-15) and therefore extremely significant. From our statistical tests, we include vehicle_age in our model and GLM3 is therefore our best model so far.

```{r}
# time consistency
compData = compData %>%
  mutate(AY_factor = as.factor(Accident_year))
glm_vehicle_age_TC <- glm(Comp_claim_count ~ AY_factor * vehicle_age, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(glm_vehicle_age_TC)
```

```{r}
effect_2018 = glm_vehicle_age_TC$coefficients['vehicle_age']
effect_2019 = glm_vehicle_age_TC$coefficients['vehicle_age'] + glm_vehicle_age_TC$coefficients['AY_factor2019:vehicle_age']
effect_2020 = glm_vehicle_age_TC$coefficients['vehicle_age'] + glm_vehicle_age_TC$coefficients['AY_factor2020:vehicle_age']

kable(data.frame(GLM3$coefficients['vehicle_age'],"2.67e-13",effect_2018,effect_2019,effect_2020), col.names=c("Coefficients", "P-value", "2018 Effect", "2019 Effect","2020 Effect"), digits = 3)
```

```{r}
AIC_BIC(GLM2)
AIC_BIC(GLM3)
anova(GLM2,GLM3, test="F")
```
# Annual distance

Next, we'll investigate whether Annual_distance makes a good predictor. There are some missing values, in this case we'll impute those values with the median of 12000. For validation purposes, the minimum and maximum have also been calculated below:
```{r}
compData %>%
  filter(partition == "Training") %>%
  pull(Annual_distance) %>%
  quantile(probs = c(0,0.5,1), na.rm = TRUE)
```
```{r}

# Replace missing values with median Annual_distance of 12000
compData = compData %>% 
  mutate(Annual_distance_missing = is.na(Annual_distance),
         Annual_distance_imputed = ifelse(Annual_distance_missing, 12000, Annual_distance))
```

There are no negative values which is a good sign, however the minimum is 1 which is low but could be possible for vehicles that are rarely driven, e.g vintage collectible cars. Let's keep those small values in for now. The maximum value also seems high. The graph of the distribution (in the appendix) can help us understand the data better:

```{r}
ggplot(data = compData %>% filter(partition == "Training" & !is.na(Annual_distance)), aes(x = Annual_distance)) + 
  geom_density() +
  geom_vline(xintercept = 12000, colour = "red")
```
The distribution is very right skewed, and annual distance reaches very large values but with low data volume. Considering right censoring at 35000, since density becomes very low after around this point (this affects 460 observations), we get the following graph:

```{r}
# There is low data volume as annual distance gets higher. Let's consider right censoring. This removes 460 observations
ggplot(data = compData %>% filter(partition == "Training" & !is.na(Annual_distance)), aes(x = Annual_distance)) + 
  geom_density() + xlim(1,35000) +
  geom_vline(xintercept = 12000, colour = "red")
```
There are 3 noticeable spikes in the data, at 12000, which is expected since we replaced the missing values with the median 12000. There are spikes at 10000 and 15000, probably stemming from the fact that this variable is manually inputted by the policyholder, so they probably gave an round estimate of annual distance driven. Overall, the data seems to be valid.

We'll fit two models, one with Annual_distance added with no capping, we'll call it GLM4a and one with Annual_distance capped at 35000 (GLM4b). We find that overall, model 4b does better, AIC and BIC both decrease from 15390.86 to 15383.76 and from 15467.92 15460.82. The p-value for GLM4b is smaller (0.000893 vs 0.000227). Thus, we conclude that we want to keep the capped annual distance in our model.

```{r}
# Without capping
GLM4a <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + insured_age_imputed + vehicle_age + Annual_distance_imputed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM4a)
```

```{r}
#Plotting functions
library(insuranceData) # Insurance data package
library(ggplot2) # For nice plots
library(grid) # For arranging plots
library(gridExtra) # For arranging plots
library(Hmisc) # For binning
#data("dataCar") # Load data, uncomment to run

#' @title Binning function
#'
#' @param response A numeric vector or data.frame. If it is a data frame, each column of response will be binned
#' @param x A vector, the explanatory variable which you will like to bin
#' @param weight A numeric vector, corresponding weights (if applicable), set to 1 or omit to have equal weights
#' @param type A string, one of "equal" for equal width bins, "quantile" for quantiles, "minimum" for a minimum number of observations per bin
#' The remaining parameters "type" and "g" can be ignored for categorical x; only set if dealing with numerical x
#' @param g If type = "equal", set g to the desired number of equal cuts.
#' If type = "quantile", set g to the desired number of quantiles.
#' If type = "minimum", set g to the minimum desired number of observations per bin.
#'
#' @return
#' @export
#'
#' @examples
binnedData <- function(response, x, weight = rep(1, length(x)), type = "equal", g = 10){
  # Check for packages
  require(data.table)
  require(Hmisc)
  #browser()
  # I need this function to compute the midpoint of an interval (e.g. midpoint of interval (3.0, 3.5] is 3.25) - please ignore - too complicated to explain
  midpoints <- function(x, dp=2){
    lower <- as.numeric(gsub(",.*","",gsub("\\(|\\[|\\)|\\]","", x)));
    upper <- as.numeric(gsub(".*,","",gsub("\\(|\\[|\\)|\\]","", x)));
    return(round(lower+(upper-lower)/2, dp));
  }

  if (is.numeric(x)){ # Check if numeric
    # Compute bins using cut2 function from Hmisc package
    binnedData = switch(type, equal = cut2(x, cuts = pretty(x, g), oneval = F),
                        quantile = cut2(x, cuts = unique(wtd.quantile(x, weights = weight, probs = (1:(g-1)/g)))), #cut2(x, g = g, oneval = F),
                        minimum = cut2(x, m = g))
    # Put all data into data.table
    binnedData = data.table("x" = binnedData, "weight" = weight)
    binnedData = cbind(binnedData, response)

    # Compute midpoints
    #print(head(binnedData))
    #binnedData[, mids := midpoints(x)]
    #binnedData = binnedData[order(mids)]
    #binnedData[, mids := NULL] # Removing it
  } else{
    # Put all data into data.table
    binnedData = data.table("x" = x, "weight" = weight)
    binnedData = cbind(binnedData, response)
  }
  # Aggregate by the binned x
  setkeyv(binnedData, "x")
  sdCols = setdiff(colnames(binnedData), c("x"))
  binnedData = binnedData[, list("weight" = sum(weight)), by = "x"][binnedData[,lapply(.SD, weighted.mean, w = weight), by = "x", .SDcols = sdCols]]
  binnedData[, i.weight := NULL]
  return (binnedData[])
}

#' @title  Plotting function
#' Similar initial parameters as binning function
#' You can choose to provide the binned data or just let the plotting function compute it (it will just call the binnedData function)
#' data = NULL if you want to let the plotting function compute the binned data.
#' Otherwise, set binnedData to the output of the binnedData function and don't bother to set the parameters response, x and weight below.
#' @param response
#' @param x
#' @param weight
#' @param type
#' @param g
#' @param data
#' @param xlab x-axis label
#' @param ylab y-axis label
#' @param wlab weight label
#' @param title graph title
#' @param plotIt
#'
#' @return
#' @export
#'
#' @examples
binnedPlot <- function(response = NULL, x = NULL, weight = rep(1, length(x)), type = "equal", g = 10, data = NULL, xlab = "Bins", ylab = "Response", wlab = "Weight", title = "Binned plot", plotIt = T, showWeights = T){
  # Check for packages
  require(data.table)
  require(Hmisc)
  require(ggplot2)
  require(gridExtra)

  if (is.null(data)){
    data = binnedData(response, x, weight, type, g)
  }
  data[, index := .I]

  p2 = ggplot(data) + # Base plot (blank)
    geom_bar(color = "black", aes(x = index, y = weight), stat = "identity", width = 1) + # Bars for weight
    labs(y = wlab, x = xlab) + # Add weight and x-axis label
    scale_x_continuous(limits = c(0.5, max(data$index) + 0.5), breaks = data$index, labels = data$x) + # Just fixing the width of the plot and adding binned labels to plot
    theme(axis.text.x = element_text(angle = 45, hjust = 1))


  data = data[, melt.data.table(data, id.vars = c("x", "weight", "index"))]
  p1 = ggplot(data) + # Base plot (blank)
    geom_line(size = 1, aes(x = index, y = value, color = variable)) + # Line plot of average response
    geom_point(size = 2, aes(x = index, y = value, color = variable)) + # Add points so that it looks like the line plot connects the points
    scale_x_continuous(limits = c(0.5, max(data$index) + 0.5)) + # Just fixing the width of the plot
    labs(y = ylab, x = NULL) + ggtitle(title) + # Add response label and title
    theme(axis.line.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), legend.position = "top") # Keep x-axis blank
  # I took the following from http://www.exegetic.biz/blog/2015/05/r-recipe-aligning-axes-in-ggplot2/ to better align the plots
  p1 = ggplot_gtable(ggplot_build(p1))
  p2 = ggplot_gtable(ggplot_build(p2))
  maxWidth = unit.pmax(p1$widths[2:3], p2$widths[2:3])
  p1$widths[2:3] = maxWidth
  p2$widths[2:3] = maxWidth
  if(plotIt){
    if (showWeights){
      grid.arrange(p1, p2, ncol = 1)
    } else{
      grid.arrange(p1)
    }
  }
  else arrangeGrob(p1, p2, ncol = 1)
}

#' @title Partial residual function
#'
#' @param mod An object of class glm, the output of glm(...)
#' @param term A string denoting the name of the predictor for which you desire the partial residuals
#'
#' @return
#' @export
#'
#' @examples
partialResids <- function(mod, term){
  partialResiduals = residuals(mod, "partial") # Get all partial residuals
  whichCols = grep(term, colnames(partialResiduals)) # Finds column names which contain the defined 'term'
  return(apply(partialResiduals[, whichCols, drop = F], 1, mean)) # Sum the partial residuals over all columns
}
```

```{r}
# Cap at 35000
compData = compData %>%
  mutate(Annual_distance_cap_35000 = pmin(Annual_distance_imputed, 35000))
```

```{r}
# With cap at 35000
GLM4b <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + insured_age_imputed + vehicle_age + Annual_distance_cap_35000, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM4b)
```

```{r}
AIC_BIC(GLM4a)
AIC_BIC(GLM4b)
```
The following graph shows the partial residuals by annual distance (capped at 35000):
```{r}
compData_train = compData %>%
  filter(partition == "Training")

working_resid = residuals(GLM4b, "working")
mean_workingResid = weighted.mean(working_resid, 
                                  w = compData_train %>% pull(Comp_earned_count))
working_resid = working_resid - mean_workingResid

modelledLine = coefficients(GLM4b)['Annual_distance_cap_35000'] * compData_train %>% pull(Annual_distance_cap_35000)

dataToPlot = data.frame("Partial Residuals" = working_resid + modelledLine,
                        "Modelled" = modelledLine)
binnedPlot(dataToPlot, 
           x = pmin(compData_train %>% pull(Annual_distance_imputed), 310000), 
           weight = compData_train %>% pull(Comp_earned_count), 
           type = "equal",
           g=12, 
           xlab = "Annual distance, imputed", 
           ylab = "Partial residuals", 
           title = "Partial residuals by annual distance")
```
The model does fairly well for annual distance which is less than ~50000, probably due to higher data volumes for annual distances less than 50000. We see an obvious drop at around 75000, thereafter our model consistently overestimates our response. Let's "zoom" in to see a more accurate picture, the following graph shows the partial residuals from [0,80000]: 

```{r}
compData_train = compData %>%
  filter(partition == "Training")

working_resid = residuals(GLM4b, "working")
mean_workingResid = weighted.mean(working_resid, 
                                  w = compData_train %>% pull(Comp_earned_count))
working_resid = working_resid - mean_workingResid

modelledLine = coefficients(GLM4b)['Annual_distance_cap_35000'] * compData_train %>% pull(Annual_distance_cap_35000)

dataToPlot = data.frame("Partial Residuals" = working_resid + modelledLine,
                        "Modelled" = modelledLine)
binnedPlot(dataToPlot, 
           x = pmin(compData_train %>% pull(Annual_distance_imputed), 80000), 
           weight = compData_train %>% pull(Comp_earned_count), 
           type = "equal",
           g=12, 
           xlab = "Annual distance, imputed", 
           ylab = "Partial residuals", 
           title = "Partial residuals by annual distance")
```
Our GLM4b model does fairly well for lower annual distances and seem to be picking up the signal quite well, there is some discrepancies where data volume becomes low but this is expected. However, we can see that after 75000, the model consistently overestimates our response (also seen in the earlier graph). We can try to introduce a hinge function at 75000. We do so by fitting another model, called GLM4c, this time with annual distance cap at 35000 and annual distance cap at 75000 added. 
```{r}
# Introducing a hinge function at 75000
compData = compData %>%
  mutate(Annual_distance_cap_75000 = pmin(Annual_distance_imputed, 75000))
compData_train = compData %>%
  filter(partition == "Training")
GLM4c <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + insured_age_imputed + vehicle_age + Annual_distance_cap_35000 + Annual_distance_cap_75000, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
```

```{r}
working_resid = residuals(GLM4c, "working")
mean_workingResid = weighted.mean(working_resid, 
                                  w = compData_train %>% pull(Comp_earned_count))
working_resid = working_resid - mean_workingResid

modelledLine = coefficients(GLM4c)['Annual_distance_cap_35000'] * compData_train %>% pull(Annual_distance_cap_35000) +
  coefficients(GLM4c)['Annual_distance_cap_75000'] * compData_train %>% pull(Annual_distance_cap_75000)

dataToPlot = data.frame("Partial Residuals" = working_resid + modelledLine,
                        "Modelled" = modelledLine)
binnedPlot(dataToPlot, 
           x = pmax(compData_train %>% pull(Annual_distance_imputed), 60000), 
           weight = compData_train %>% pull(Comp_earned_count), 
           type = "equal",
           g=12, 
           xlab = "Annual distance, imputed", 
           ylab = "Partial residuals", 
           title = "Partial residuals by annual distance")
```
We checked the partial residuals graph, the hinge function reduces the distance between the modelled line and the partial residuals line but the model still slightly overfits the response (see appendix). We then compare the fit between model 4b and 4c using a **parsimony** test, we find that both AIC and BIC increase when we add the hinge function (GLM4c) from 15383.76 to 15385.49 and 15460.82 to 15472.19, respectively. The F test also suggest that GLM4b is a better fit with a p value of 0.657. Thus, so far our best model is GLM4b.

```{r}
AIC_BIC(GLM4b)
AIC_BIC(GLM4c)
anova(GLM4b,GLM4c, test = "F")
```
# Comp earned count & Direct comp earned count

We investigate Comp_earned_count by fitting a model with Comp_earned_count added to our GLM4b model (calling the new model GLM5) and fit another model with Direct_comp_earned_count added to GLM4b (calling this model GLM6). We decide **not** to include both variables. Comp_earned_count fails the BIC test as it increases from 15460.82 to 15468.29, the F-test was also not significant at 5% significance level when comparing GLM4b to GLM6 (p-value=0.06808). Direct_comp_earned count fails the F-test with a p value of 0.1431, as well as the BIC test since BIC increases from 15460.82 (for GLM4b) to 15469.62 (for GLM6).

```{r}
GLM5 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + insured_age_imputed + vehicle_age + Annual_distance_cap_35000 + Comp_earned_count, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)

summary(GLM5)
AIC_BIC(GLM4b)
AIC_BIC(GLM5)
```

```{r}
anova(GLM4b,GLM5, test = "F")
```

```{r}
GLM6 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + insured_age_imputed + vehicle_age + Annual_distance_cap_35000 + Direct_comp_earned_count, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)

summary(GLM6)
```

```{r}
AIC_BIC(GLM4b)
AIC_BIC(GLM6)
anova(GLM4b, GLM6, test = "F")
```
# Number of years licensed

We now create a num_yrs_licensed variable, this is the number of years that the insured has had their license for. We do so by calculating the difference between the insured year licensed and the year of the term effective date. Note that in this case, there were no missing values, so no additional adjustments were made to the data. We fit a glm model with num_yrs_licensed along with the other predictors from GLM4b, we'll call this new model GLM7a.

```{r}
# Creating num_yrs_licensed variable
compData = compData %>%
  mutate(num_yrs_licensed = as.numeric(substr(Term_effective_date,0,4)) - Insured_year_licensed)
```


```{r}
GLM7a <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + insured_age_imputed + vehicle_age + Annual_distance_cap_35000 + num_yrs_licensed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)

summary(GLM7a)
```
When we fit the model,num_yrs_licensed is **significant** with a p-value of 0.023883. Checking for **reasonability**, the coefficient for num_yrs_licensed is negative which is expected as we expect that more experienced drivers have lower claim frequency, this is a reasonable assumption and a commonly used rating variable in the insurance industry. For a **time consistency** check, we find that the variable is consistent across the three years (see effects plot in the appendix). Finally, for **parsimony**, AIC decreases but BIC increases compared to model 4b.

```{r}
# time consistency
compData = compData %>%
  mutate(AY_factor = as.factor(Accident_year))
glm_num_yrs_licensed_TC <- glm(Comp_claim_count ~ AY_factor * num_yrs_licensed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(glm_num_yrs_licensed_TC)

library(effects)
glm_num_yrs_licensed_TC$coefficients['num_yrs_licensed']
glm_num_yrs_licensed_TC$coefficients['num_yrs_licensed'] + glm_num_yrs_licensed_TC$coefficients['AY_factor2019:num_yrs_licensed']
glm_num_yrs_licensed_TC$coefficients['num_yrs_licensed'] + glm_num_yrs_licensed_TC$coefficients['AY_factor2020:num_yrs_licensed']
plot(allEffects(glm_num_yrs_licensed_TC), multiline=T)
```

```{r}
kable(data.frame(GLM7a$coefficients['num_yrs_licensed'],0.023883,effect_2018,effect_2019,effect_2020), col.names=c("Coefficients", "P-value", "2018 Effect", "2019 Effect","2020 Effect"), digits = 6)
```

```{r}
# Checking for correlation between insured age and number of years licensed
cor(compData$insured_age_imputed,
    compData$num_yrs_licensed,
    method = "pearson") 

cor(compData$insured_age_imputed,
    compData$num_yrs_licensed,
    method = "spearman")
```
One important thing to note is that we find that insured_age is not significant anymore, this is due to the strong correlation between age and number of years licensed. More specifically, insured_age and num_yrs_licensed had a Pearson correlation of 0.91396 and Spearman correlation of 0.91445. This is also probably why our BIC increased in our parsimony test. Therefore, we will only keep one of the two variables in our model.

To decide which variable to keep, we fit the GLM7b model which contains num_yrs_licensed and all our predictors so far except insured_age (i.e factor(Accident_year), Marital_status, vehicle_age, Annual_distance_cap_35000). The following table shows the AIC and BIC of models with both variables, with only insured_age added, and with only num_yrs_licensed added.

```{r}
GLM7b <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age +
               Annual_distance_cap_35000 + num_yrs_licensed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM7b)
```

```{r}
AIC_BIC_4b = AIC_BIC(GLM4b)
AIC_BIC_7a = AIC_BIC(GLM7a)
AIC_BIC_7b = AIC_BIC(GLM7b)
```

```{r}
kable(data.frame(AIC_BIC_4b,AIC_BIC_7a,AIC_BIC_7b), col.names=c("Insured_age only (GLM4b) - AIC", "Insured_age only (GLM4b) - BIC","Both variables (GLM7a)- AIC","Both variables (GLM7a)- BIC", "num_yrs_licensed only (GLM7b) - AIC", "num_yrs_licensed only (GLM7b) - BIC"))

```

We can see that both AIC and BIC are their lowest for model 7b. Therefore, it suggests that we should **exclude insured_age but include num_yrs_licensed** instead. To validate this even further, we perform an F-test between GLM7a (contains both variables) and GLM7b (excludes insured_age). The p-value is 0.4582 which shows **no evidence that insured_age should also be included**.

```{r}
anova(GLM7a, GLM7b, test = "F")
```

# Urban or Rural Class

Next, we created urban_rural_class, this classifies risks accordingly to whether their vehicle is stored in a rural or urban area. To create this variable, we used risk postal code instead of insured postal code, since intuitively it makes more sense to use the risk postal code since it is where the vehicle is stored. However, risk_postal_code had some missing values, so we replaced those values with insured_postal_code. A risk would be classified as "Rural" if the first digit in the postal code is "0" and classified as "Urban" otherwise.

We fit a new model with urban_rural_class added. However, while it seemed intuitive that claim frequency could depend on territorial regions, urban_rural_class failed the **significance** test with a p value of 0.214. We exclude this variable from our model, and GLM7b remains our best model so far.

```{r}
# Create urban_rural_class variable
compData = compData %>%
  mutate(Risk_postal_code_missing = (Risk_postal_code == ""),
         Risk_postal_code_imputed = ifelse(Risk_postal_code_missing, Insured_postal_code, Risk_postal_code)) %>%
  mutate(urban_rural_class = ifelse(substr(Risk_postal_code_imputed,2,2) == "0", "Rural", "Urban"))
```

```{r}
GLM8 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age +
               Annual_distance_cap_35000 + num_yrs_licensed + urban_rural_class, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM8)
```

# Vehicle retail price & Vehicle wheelbase

In this section, we will investigate both Vehicle retail price & Vehicle wheelbase since we find that adding both to our model (GLM10a) makes Vehicle retail price not significant anymore. Similar to insured_age and num_yrs_licensed, we have included a table below with the AIC and BIC for models with both variables, and with each one of the variables only.

We impute missing values for Vehicle_retail_price with the median. We then fit a model (GLM9) with the Vehicle_retail_price added to our previous model. Vehicle_retail_price passes the **significance** test with a p value of 0.042342. For our **parismony** check, we find that AIC decreases slightly, but BIC increases. It passes the F-test with a p value of 0.0476 when we compare a model with Vehicle_retail_price and our previous best model. Therefore, including Vehicle_retail_price only could lead to a better fit.

We impute missing values for Vehicle_wheelbase with the median. We then fit a model (GLM10b) with the Vehicle_wheelbase added to our previous model 7b. Vehicle_wheelbase passes the **significance** test with p value of 0.004973.

Hence, from the table below, we can see that AIC and BIC are the lowest for model 10b. The F-test further supports this argument because of the large p value of 0.241 when comparing nested models 10a (includes both variables) and GLM10b (only includes Vehicle_wheelbase). Thus, we conclude that we want to **exclude Vehicle_retail_price and include Vehicle_wheelbase** instead.

```{r}
# Vehicle retail price
compData %>%
  filter(partition == "Training") %>%
  pull(Vehicle_retail_price) %>%
  quantile(probs = c(0,0.5,1), na.rm = TRUE)
```

```{r}
# Replace missing values with median retail price of 30388
compData = compData %>% 
  mutate(Vehicle_retail_price_missing = is.na(Vehicle_retail_price),
         Vehicle_retail_price_imputed = ifelse(Vehicle_retail_price_missing, 30388, Vehicle_retail_price))
```

```{r}
GLM9 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age +
               Annual_distance_cap_35000 + num_yrs_licensed + Vehicle_retail_price_imputed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM9)
```

```{r}
AIC_BIC(GLM7b)
AIC_BIC(GLM9)
anova(GLM7b,GLM9, test = "F")
```

```{r}
# Vehicle_wheelbase
compData %>%
  filter(partition == "Training") %>%
  pull(Vehicle_wheelbase) %>%
  quantile(probs = c(0,0.5,1), na.rm = TRUE)
```

```{r}
# Replace missing values with median Vehicle_wheelbase of 2800
compData = compData %>% 
  mutate(Vehicle_wheelbase_missing = is.na(Vehicle_wheelbase),
         Vehicle_wheelbase_imputed = ifelse(Vehicle_wheelbase_missing, 2800, Vehicle_wheelbase))
```

```{r}
GLM10a <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age +
               Annual_distance_cap_35000 + num_yrs_licensed + Vehicle_retail_price_imputed + Vehicle_wheelbase_imputed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM10a)
```

```{r}
# Checking for correlation
cor(compData$Vehicle_retail_price_imputed,
    compData$Vehicle_wheelbase_imputed,
    method = "pearson") 

cor(compData$Vehicle_retail_price_imputed,
    compData$Vehicle_wheelbase_imputed,
    method = "spearman")
```

```{r}
GLM10b <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age +
               Annual_distance_cap_35000 + num_yrs_licensed + Vehicle_wheelbase_imputed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM10b)
```

```{r}
AIC_BIC_9 = AIC_BIC(GLM9)
AIC_BIC_10a = AIC_BIC(GLM10a)
AIC_BIC_10b = AIC_BIC(GLM10b)

kable(data.frame(AIC_BIC_4b,AIC_BIC_7a,AIC_BIC_7b), col.names=c("Vehicle_retail_price only (GLM9) - AIC", "Vehicle_retail_price only (GLM9) - BIC","Both variables (GLM10a)- AIC","Both variables (GLM10a)- BIC", "Vehicle_wheelbase only (GLM10b) - AIC", "Vehicle_wheelbase only (GLM10b) - BIC"))
```

```{r}
anova(GLM10a, GLM10b, test = "F")
```

# Vehicle make classification

Let's check if adding Vehicle_make_classification improves our current model. Assessing **significance**, we find that a lot of the bins are not significant (ie the p values are quite large). From a **reasonability** standpoint, it is difficult to justify using this variable for predicting comprehensive claim frequency since it is based on “expected collision loss cost”. For a **parsimony** check, both AIC and BIC increases when Vehicle_make_classification is added, from 15370.02 to 15383.1 for AIC and from 15456.71 to 15729.89 for BIC. However, this large increase in BIC would be expected since when adding Vehicle_make_classification, we add a lot of new parameters which heavily penalises BIC. We also perform an F-test to compare GLM10b and GLM11, we get a p-value of 0.3133, hence there is no evidence that adding Vehicle_make_classification to our model would improve its predictive abilities. From these tests already, we can conclude that we should not include this variable in our model, therefore we did not perform **time consistency** tests for conciseness since we would probably still exclude this variable even if it was consistent across time. 

```{r}
GLM11 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age +
               Annual_distance_cap_35000 + num_yrs_licensed + Vehicle_wheelbase_imputed + Vehicle_make_classification, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM11)
```

```{r}
compData %>% 
  filter(partition == "Training") %>%
  group_by(Vehicle_make_classification) %>%
  summarise(exposure = sum(Comp_earned_count),
            frequency = sum(Comp_claim_count) / sum(Comp_earned_count))
```

#```{r}
#round(compData$Vehicle_make_classification, b, method="pearson"), 2) 
#```

```{r}
dataToPlot = compData %>%
  filter(partition == "Training") %>%
  mutate(freq = Comp_claim_count / Comp_earned_count)
binnedPlot(response = dataToPlot$freq,
           x = dataToPlot$Vehicle_make_classification, 
           weight = dataToPlot$Comp_earned_count, 
           type="equal", 
           g=12, 
           xlab = "Vehicle make classification", 
           ylab = "Claim Frequency", 
           title = "Observed claim frequency by Vehicle make classification")
```

```{r}
anova(GLM10b,GLM11, test = "F")
AIC_BIC(GLM10b)
AIC_BIC(GLM11)
```
# Vehicle horsepower

Next we tried adding Vehicle_horsepower to our model after imputing the missing values with the median from our training data. Hwoever, we find that vehicle_horsepower was not significant with a large p value of 0.77003. Therefore, we decided to exclude it from our model.

```{r}
# Vehicle_horsepower
compData %>%
  filter(partition == "Training") %>%
  pull(Vehicle_horsepower) %>%
  quantile(probs = c(0,0.5,1), na.rm = TRUE)
```

```{r}
# Replace missing values with median Vehicle_horsepower of 190
compData = compData %>% 
  mutate(Vehicle_horsepower_missing = is.na(Vehicle_horsepower),
         Vehicle_horsepower_imputed = ifelse(Vehicle_horsepower_missing, 190, Vehicle_horsepower))
```

```{r}
GLM14 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age +
               Annual_distance_cap_35000 + num_yrs_licensed + Vehicle_wheelbase_imputed + Vehicle_horsepower_imputed, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM14)
```

# Comp deductible

Next, we try fitting Comp_deductible in our model as a categorical variable taking levels 100, 500 and 1000. We find that the majority of exposures were in the 500 level (see appendix), so we don't know if that would be a good variable to include due to the low data volume in the other levels. Nonetheless, let's perform the usual statistical tests. 

```{r}
# Make Comp_deductible a string to take as categorical variable)
compData = compData %>% 
  mutate(Comp_deductible_ascat = as.character(Comp_deductible))
```

```{r}
compData %>% 
  filter(partition == "Training") %>%
  group_by(Comp_deductible_ascat) %>%
  summarise(exposure = sum(Comp_earned_count),
            frequency = sum(Comp_claim_count) / sum(Comp_earned_count))
```
For the **significance** test, We find that the p value is small for the 500 level but higher for the 1000 level. From a **reasonability** standpoint, using comp_deductible as a rating variable is possible since it policyholders with higher deductible levels will only report claims with large loss amounts resulting in lower claim frequency, and we have seen this pattern in the table above. For **parsimony**, AIC decreases from 15370.02 to 15362.94 but BIC increases from 15456.71 to 15468.9. Performing a **time consistency** check, we see that the 500 level is consistent across the three years but the 1000 level isn't. We are more concerned that the 500 level is consistent since it has the majority of exposures. Finally, let's perform an F test, the p value is small (0.01479), so there is evidence that adding Comp_deductible improves our fit. Although there isn't a clear cut from our tests whether this variable should be included or not. Let's keep it for now and we'll perform more validation later on the holdout set.

```{r}
GLM15 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age + Annual_distance_cap_35000 + num_yrs_licensed + Vehicle_wheelbase_imputed + Comp_deductible_ascat,
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM15)
```

```{r}
anova(GLM10b,GLM15, test = "F")
AIC_BIC(GLM10b)
AIC_BIC(GLM15)
```

```{r}
# time consistency
compData = compData %>%
  mutate(AY_factor = as.factor(Accident_year))
glm_Comp_deductible_ascat_TC <- glm(Comp_claim_count ~ AY_factor * Comp_deductible_ascat, 
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(glm_Comp_deductible_ascat_TC)
```

```{r}

effect_2018_1000 = glm_Comp_deductible_ascat_TC$coefficients['Comp_deductible_ascat1000'] 
effect_2019_1000 = glm_Comp_deductible_ascat_TC$coefficients['Comp_deductible_ascat1000'] + glm_Comp_deductible_ascat_TC$coefficients['AY_factor2019:Comp_deductible_ascat1000']
effect_2020_1000 = glm_Comp_deductible_ascat_TC$coefficients['Comp_deductible_ascat1000'] + glm_Comp_deductible_ascat_TC$coefficients['AY_factor2020:Comp_deductible_ascat1000']

effect_2018 = glm_Comp_deductible_ascat_TC$coefficients['Comp_deductible_ascat500']
effect_2019 = glm_Comp_deductible_ascat_TC$coefficients['Comp_deductible_ascat500'] + glm_Comp_deductible_ascat_TC$coefficients['AY_factor2019:Comp_deductible_ascat500']
effect_2020 = glm_Comp_deductible_ascat_TC$coefficients['Comp_deductible_ascat500'] + glm_Comp_deductible_ascat_TC$coefficients['AY_factor2020:Comp_deductible_ascat500']

kable(data.frame(c(GLM15$coefficients['Comp_deductible_ascat1000'],GLM15$coefficients['Comp_deductible_ascat500']),c(0.565786,0.023139),c(effect_2018_1000, effect_2018),c(effect_2019_1000, effect_2019),c(effect_2020_1000, effect_2020)), col.names=c("Coefficients", "P-value", "2018 Effect", "2019 Effect","2020 Effect"), digits = 3)
```

# Years since purchased

We create a new variable called yrs_since_purchased which is the number of years since the insured purchased the vehicle. Hence, we calculate the difference between the year of the term effective date and the vehicle purchase year. Again, we impute missing values with the median (3 in this case).

Again we perform our usual tests after fitting the model which we call GLM16. We find that the variable is significant (p value = 0.014285) and the F-test shows a small p value of 0.01103 too. The added variable passes the AIC test but fails the BIC test. Overall, the variable improves the model fit, so we include it for now.

```{r}
# Creating yrs_since_purchased variable
compData = compData %>%
  mutate(yrs_since_purchased = as.numeric(substr(Term_effective_date,0,4)) - as.numeric(substr(Vehicle_purchase_date,0,4)))
```

```{r}
# yrs_since_purchased
compData %>%
  filter(partition == "Training") %>%
  pull(yrs_since_purchased) %>%
  quantile(probs = c(0,0.5,1), na.rm = TRUE)
```

```{r}
compData = compData %>%
  mutate(yrs_since_purchased_missing = is.na(yrs_since_purchased),
         yrs_since_purchased_imputed = ifelse(yrs_since_purchased_missing, 3, yrs_since_purchased))
```

```{r}
GLM16 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age + Annual_distance_cap_35000 + num_yrs_licensed + Vehicle_wheelbase_imputed + Comp_deductible_ascat + yrs_since_purchased_imputed,
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM16)
```

```{r}
anova(GLM15,GLM16, test = "F")
AIC_BIC(GLM15)
AIC_BIC(GLM16)
```
# Years with company
Lastly, we create a new variable, yrs_with_company, which is the number of years someone has been a policyholder with us. However, this variable turned out to not be significant as it had a large p value of 0.118689, so we didn't include it.

```{r}
# Creating yrs_with_company variable
compData = compData %>%
  mutate(yrs_with_company = as.numeric(substr(Term_effective_date,0,4)) - as.numeric(substr(Inception_date,0,4)))
```

```{r}
# yrs_with_company
compData %>%
  filter(partition == "Training") %>%
  pull(yrs_with_company) %>%
  quantile(probs = c(0,0.5,1), na.rm = TRUE)
```

```{r}
compData = compData %>%
  mutate(yrs_with_company_missing = is.na(yrs_with_company),
         yrs_with_company_imputed = ifelse(yrs_with_company_missing, 6, yrs_with_company))
```

```{r}
GLM17 <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age + Annual_distance_cap_35000 + num_yrs_licensed + Vehicle_wheelbase_imputed + Comp_deductible_ascat + yrs_since_purchased_imputed + yrs_with_company_imputed,
                family=quasipoisson(link="log"), 
                data = compData, 
                offset=log(Comp_earned_count), 
                subset=(partition=="Training"), 
                na.action="na.pass",
                x = TRUE)
summary(GLM17)
```

```{r}
# Function to save predictions
savePredictions <- function(glmModel, kaggleSubmission, saveLocation = getwd()){
    # Save time
    saveTime = format(Sys.time(), "%Y%m%d_%H%M%S")

    # Make GLM smaller
    class(glmModel) = c(class(glmModel), "list")
    for (i in c("y", "residuals", "fitted.values", "linear.predictors", "weights", "prior.weights", "data")){
        glmModel[[i]] = head(glmModel[[i]], 100)
    }
    glmModel$effects = c()
    glmModel$qr = c() 

    # Save model
    saveRDS(glmModel, file = file.path(saveLocation, paste0("model_", saveTime, ".RDS")))

    # Save predictions
    write.csv(kaggleSubmission, 
              file = file.path(saveLocation, paste0("submission_", saveTime, ".csv")), row.names = F)  

    cat("Saved two files:", paste0("model_", saveTime, ".RDS"), " and ", paste0("submission_", saveTime, ".csv"), " at ", saveLocation, "\n")
}
```

```{r}
# # Import data
# require(data.table)
# train = fread("compDatasetTrain1.csv")
# test = fread("compDatasetTest.csv")
# # Construct intercept-only model
# # glmMod = glm(Comp_claim_count ~ 1, data = train, family = poisson(link = "log"), offset = log(Comp_earned_count))
# 
# # Order the test dataset by id, and create/modify required variables
# test = test[order(test$id),]
# test = test %>%
#   mutate(Accident_year = "2020") %>%
#   mutate(insured_age = as.numeric(substr(Term_effective_date,0,4)) - as.numeric(substr(Insured_birth_date,0,4))) %>%
#   mutate(Marital_status = case_when(
#     Marital_status == "Single" ~ "Single",
#     Marital_status == "Widowed" ~ "Single",
#     Marital_status == "Divorced" ~ "Single",
#     Marital_status == "Separated" ~ "Single",
#     Marital_status == "Common Law" ~ "Married",
#     Marital_status == "Same sex" ~ "Married",
#     Marital_status == "Married" ~ "Married",
#     Marital_status == "" ~ "Missing")) %>%
#   mutate(vehicle_age = as.numeric(substr(Term_effective_date,0,4)) - Vehicle_model_year) %>%
#   mutate(Annual_distance = pmin(Annual_distance, 35000)) %>%
#   mutate(num_yrs_licensed = as.numeric(substr(Term_effective_date,0,4)) - Insured_year_licensed) %>%
#   mutate(Comp_deductible_ascat = as.character(Comp_deductible)) %>%
#   mutate(yrs_since_purchased = as.numeric(substr(Term_effective_date,0,4)) - as.numeric(substr(Vehicle_purchase_date,0,4)))
# 
# train = train %>%
#   mutate(insured_age = as.numeric(substr(Term_effective_date,0,4)) - as.numeric(substr(Insured_birth_date,0,4))) %>%
#   mutate(Marital_status = case_when(
#     Marital_status == "Single" ~ "Single",
#     Marital_status == "Widowed" ~ "Single",
#     Marital_status == "Divorced" ~ "Single",
#     Marital_status == "Separated" ~ "Single",
#     Marital_status == "Common Law" ~ "Married",
#     Marital_status == "Same sex" ~ "Married",
#     Marital_status == "Married" ~ "Married",
#     Marital_status == "" ~ "Missing")) %>%
#   mutate(vehicle_age = as.numeric(substr(Term_effective_date,0,4)) - Vehicle_model_year) %>%
#   mutate(Annual_distance = pmin(Annual_distance, 35000)) %>%
#   mutate(num_yrs_licensed = as.numeric(substr(Term_effective_date,0,4)) - Insured_year_licensed) %>%
#   mutate(Comp_deductible_ascat = as.character(Comp_deductible)) %>%
#   mutate(yrs_since_purchased = as.numeric(substr(Term_effective_date,0,4)) - as.numeric(substr(Vehicle_purchase_date,0,4)))
# 
# median_vehicle_age = median(c(train$vehicle_age, test$vehicle_age), na.rm = T)
# train[is.na(train$vehicle_age), "vehicle_age"] = median_vehicle_age
# test[is.na(test$vehicle_age), "vehicle_age"] = median_vehicle_age
# 
# median_Annual_distance = median(c(train$Annual_distance, test$Annual_distance), na.rm = T)
# train[is.na(train$Annual_distance), "Annual_distance"] = median_Annual_distance
# test[is.na(test$Annual_distance), "Annual_distance"] = median_Annual_distance
# 
# median_insured_age = median(c(train$insured_age, test$insured_age), na.rm = T)
# train[is.na(train$insured_age), "insured_age"] = median_insured_age
# test[is.na(test$insured_age), "insured_age"] = median_insured_age
# 
# median_num_yrs_licensed = median(c(train$num_yrs_licensed, test$num_yrs_licensed), na.rm = T)
# train[is.na(train$num_yrs_licensed), "num_yrs_licensed"] = median_num_yrs_licensed
# test[is.na(test$num_yrs_licensed), "num_yrs_licensed"] = median_num_yrs_licensed
# 
# median_Vehicle_wheelbase = median(c(train$Vehicle_wheelbase, test$Vehicle_wheelbase), na.rm = T)
# train[is.na(train$Vehicle_wheelbase), "Vehicle_wheelbase"] = median_Vehicle_wheelbase
# test[is.na(test$Vehicle_wheelbase), "Vehicle_wheelbase"] = median_Vehicle_wheelbase
# 
# median_yrs_since_purchased = median(c(train$yrs_since_purchased, test$yrs_since_purchased), na.rm = T)
# train[is.na(train$yrs_since_purchased), "yrs_since_purchased"] = median_yrs_since_purchased
# test[is.na(test$yrs_since_purchased), "yrs_since_purchased"] = median_yrs_since_purchased
#   
# GLM_to_test <- glm(Comp_claim_count ~ factor(Accident_year) + Marital_status + vehicle_age + Annual_distance + insured_age + Vehicle_wheelbase + Comp_deductible_ascat + yrs_since_purchased,
#                 family=quasipoisson(link="log"), 
#                 data = train, 
#                 offset=log(Comp_earned_count), 
#                 na.action="na.pass",
#                 x = TRUE)
# 
# summary(GLM_to_test)
# 
# # Compute predictions - predicted frequency is equal to the predicted comprehensive claim count divided by the comprehensive earn count
# predictions = predict(GLM_to_test, newdata = test, type = "response")/test$Comp_earned_count
# # Save predictions
# kaggleSubmission = data.frame("id" = test$id, "predictions" = predictions)
# savePredictions(GLM_to_test, kaggleSubmission, saveLocation = getwd())
```

```{r}
WeightedGini <- function(solution, weights = NULL, submission){
  if (is.null(weights)){
    weights = rep(1, length(solution))
  }
  df = data.frame(solution = solution, weights = weights, submission = submission)
  df <- df[order(df$submission, decreasing = TRUE),]
  df$random = cumsum((df$weights/sum(df$weights)))
  totalPositive <- sum(df$solution * df$weights)
  df$cumPosFound <- cumsum(df$solution * df$weights)
  df$Lorentz <- df$cumPosFound / totalPositive
  n <- nrow(df)
  gini <- sum(df$Lorentz[-1]*df$random[-n]) - sum(df$Lorentz[-n]*df$random[-1])
  return(gini)
}

NormalizedWeightedGini <- function(solution, weights = NULL, submission) {
  WeightedGini(solution, weights, submission) / WeightedGini(solution, weights, solution)
}

mae <- function(solution, weights = NULL, submission){
  if (is.null(weights)){
    weights = rep(1, length(solution))
  }
  return (mean(weights*abs(solution - submission)))
}

rmse <- function(solution, weights = NULL, submission){
  if (is.null(weights)){
    weights = rep(1, length(solution))
  }
  return (sqrt(mean(weights*(solution - submission)^2)))
}

computeAllMeasures <- function(solution, weights = NULL, submission){
  results = data.table("MAE" = mae(solution, weights, submission),
                          "RMSE" = rmse(solution, weights, submission),
                          "Gini" = NormalizedWeightedGini(solution, weights, submission))
  return (results)
}


territorialSmoothing <- function(data, dataRegionVar, shapeFile, shapeFileRegionVar, glmMod, responseVar, weightVar = NULL, folds = NULL, smoothingFactor = 1, clusters = 5){
    
  setDT(data)
  
  #shapeFileFortified = fortify(shapeFile, region = shapeFileRegionVar)

  shapeFileAdjacencies = gTouches(shapeFile, byid=TRUE)
  diag(shapeFileAdjacencies) = T

  data[, terr_resid := get(responseVar)/predict(glmMod, newdata = data, type = "response")]

  prevGini = 0
  currGini = 0

  for (i in 1:length(folds)){
    selTest = folds[[i]]
    selTrain = as.numeric(unlist(folds[-i]))
    
    # Summarize residuals for in-sample folds by regionVar
    if (is.null(weightVar)){
      datSum = data[selTrain, list("meanResid" = mean(terr_resid), "weight" = length(terr_resid)), by = dataRegionVar]
    } else{
      datSum = data[, list("meanResid" = weighted.mean(terr_resid, get(weightVar)), "weight" = sum(get(weightVar))), by = dataRegionVar]
    }

    datSmooth = data.table("id" = shapeFile@data[, shapeFileRegionVar]) %>% unique
    datSmooth = merge(datSmooth, datSum, by.x = "id", by.y = dataRegionVar, all.x = T, sort = F)
    datSmooth[is.na(meanResid), meanResid := 0]
    datSmooth[is.na(weight), weight := 0]

    smoothResid = lapply(1:nrow(shapeFileAdjacencies), function(x){
      currentAdjacencies = shapeFileAdjacencies[x,]
      if (sum(currentAdjacencies) > 0){
        adjacencyStats = datSmooth[currentAdjacencies == T, list("weight" = sum(weight),
                                                                 "meanResid" = weighted.mean(meanResid, weight))]
      } else{
        adjacencyStats = data.table("weight" = 0, "meanResid" = 0)
      }
      currWeight = datSmooth[x, weight][[1]]
      curr = datSmooth[x, meanResid][[1]]
      w = currWeight/(currWeight + smoothingFactor)
      if (is.na(w)){
        w = 0
      }
      w*curr + (1 - w)*adjacencyStats$meanResid
    })

    datSmooth[, terr_smoothResid := unlist(smoothResid)]
    set.seed(100)

    kmeans_clustering = kmeans(datSmooth[, terr_smoothResid], centers = clusters)
    datSmooth[, terr_cluster := round(kmeans_clustering$centers[kmeans_clustering$cluster], 2)]

    data[, terr_smoothResid := NULL]
    data[, terr_cluster := NULL]
    data = merge(data, datSmooth[, .(id, terr_smoothResid, terr_cluster)], by.x = dataRegionVar, by.y = "id", all.x = T, sort = F)

    prevMod = update(glmMod, data = data[selTrain,])
    currMod = update(glmMod, formula = . ~ . + terr_cluster, data = data[selTrain,])
    prevPred = predict(prevMod, newdata = data[selTest,])
    currPred = predict(currMod, newdata = data[selTest,])
    prevGini = prevGini + NormalizedWeightedGini(data[[responseVar]][selTest], rep(1, length(selTest)), prevPred)
    currGini = currGini + NormalizedWeightedGini(data[[responseVar]][selTest], rep(1, length(selTest)), currPred)
    #cat("\nFold #", i, ". The previous is: ", prevGini, ". The current is: ", currGini, ". The difference is: ", currGini - prevGini, ".\n")
  }

  prevGini = prevGini/length(folds)
  currGini = currGini/length(folds)

  cat("\nThe cross-validated increase in Gini with the addition of territorial clusters \nusing k = ", clusters, " and J = ", smoothingFactor, " is ", currGini - prevGini, "\n")

  setDF(data)

  return (currGini)
}

territorialSmoothingClusters <- function(data, dataRegionVar, shapeFile, shapeFileRegionVar, glmMod, responseVar, weightVar = NULL, smoothingFactor = 1, clusters = 5){
  
  setDT(data)
  
  #shapeFileFortified = fortify(shapeFile, region = shapeFileRegionVar)
  
  shapeFileAdjacencies = gTouches(shapeFile, byid=TRUE)
  diag(shapeFileAdjacencies) = T
  
  data[, terr_resid := get(responseVar)/predict(glmMod, newdata = data, type = "response")]
  
  # Summarize residuals for in-sample folds by regionVar
  if (is.null(weightVar)){
    datSum = data[, list("meanResid" = mean(terr_resid), "weight" = length(terr_resid)), by = dataRegionVar]
  } else{
    datSum = data[, list("meanResid" = weighted.mean(terr_resid, get(weightVar)), "weight" = sum(get(weightVar))), by = dataRegionVar]
  }
  
  datSmooth = data.table("id" = shapeFile@data[, shapeFileRegionVar])
  datSmooth = merge(datSmooth, datSum, by.x = "id", by.y = dataRegionVar, all.x = T, sort = F)
  datSmooth[is.na(meanResid), meanResid := 0]
  datSmooth[is.na(weight), weight := 0]
  
  smoothResid = lapply(1:nrow(shapeFileAdjacencies), function(x){
    currentAdjacencies = shapeFileAdjacencies[x,]
    if (sum(currentAdjacencies) > 0){
      adjacencyStats = datSmooth[currentAdjacencies == T, list("weight" = sum(weight),
                                                               "meanResid" = weighted.mean(meanResid, weight))]
    } else{
      adjacencyStats = data.table("weight" = 0, "meanResid" = 0)
    }
    currWeight = datSmooth[x, weight][[1]]
    curr = datSmooth[x, meanResid][[1]]
    w = currWeight/(currWeight + smoothingFactor)
    if (is.na(w)){
      w = 0
    }
    w*curr + (1 - w)*adjacencyStats$meanResid
  })
    
  datSmooth[, terr_smoothResid := unlist(smoothResid)]
  set.seed(100)
    
  kmeans_clustering = kmeans(datSmooth[, terr_smoothResid], centers = clusters)
  datSmooth[, terr_cluster := round(kmeans_clustering$centers[kmeans_clustering$cluster], 2)]
  
  data[, terr_smoothResid := NULL]
  data[, terr_cluster := NULL]
  
  setDF(data)
  
  return (datSmooth[, .(id, terr_cluster)])
}

# Cross-validate a tweedie model
cvFunction <- function(formula, dat, p = 1.5, numFolds = 5, seed = 123){
  # Import required packages
  suppressPackageStartupMessages(require(dplyr))
  suppressPackageStartupMessages(require(modelr))
  suppressPackageStartupMessages(require(purrr))
  suppressPackageStartupMessages(require(broom))
  suppressPackageStartupMessages(require(tidyr))
  
  datToTrain = dat %>% filter(partition == "Training") %>% setDT
  
  # Set seed before performing cross validation for reproducibility
  set.seed(seed)
  foldID = sample(1:numFolds, size = nrow(datToTrain), replace = T)
  
  # Perform cross validation procedure
  # Create storage for the Gini coefficients, CV models and CV predictions
  res = rep(0, numFolds)
  models = vector(mode = "list", numFolds)
  predictions = vector(mode = "list", numFolds)
  
  # Loop over folds, fit a model, predict and compute Gini 
  for (i in 1:numFolds){
    models[[i]] = glm(formula, family=tweedie(var.power = p, link.power = 0), 
                      data = datToTrain[foldID != i], weights = exposure)
    predictions[[i]] = predict(models[[i]], newdata = datToTrain[foldID == i], type = "response")
    res[i] = NormalizedWeightedGini(datToTrain[foldID == i, loss_cost], datToTrain[foldID == i, exposure], predictions[[i]])
  }
  
  # Compute summary statistics for parameter estimates
  paramStats = lapply(models, broom::tidy) %>% # Compute statistics 
    bind_rows() %>%
    group_by(term) %>% # Group by term in model
    summarise(Minimum = min(estimate), # Minimum 
              Maximum = max(estimate), # Maximum
              Mean = mean(estimate), # Mean
              Standard.Error = sd(estimate)/sqrt(numFolds) # Standard error
    ) %>%
    as.data.table()
  
  # Obtain cross-validated predictions for each fold
  datWithPredictions = datToTrain[, .(loss_cost, exposure, .id = foldID, .fitted = numeric(.N))]
  for (i in 1:numFolds){
    datWithPredictions[.id == i, .fitted := predictions[[i]]]
  }

  # Compute the Gini coefficient for each fold (.id)
  # Then compute the average (cross-validated) Gini as well as a standard error for the Gini
  cvStats = datWithPredictions %>% group_by(.id) %>% 
    summarise(Gini = NormalizedWeightedGini(loss_cost, exposure, .fitted)) %>%
    summarise(CVGini = mean(Gini), seGini = sd(Gini)/sqrt(numFolds)) %>%
    as.data.table()
  
  return(list("Parameter statistics" = paramStats,
              "Cross-validated statistics" = cvStats))
}


```

```{r}
compData_holdout = compData %>%
  filter(partition == "Holdout")
holdout_test_data = data.frame(observed_count = compData_holdout %>% pull(Comp_claim_count),
                               exposure = compData_holdout %>% pull(Comp_earned_count),
                               observed_frequency = compData_holdout %>% pull(Comp_claim_count) / compData_holdout %>% pull(Comp_earned_count),
                               prediction_option_1 = predict(GLM10b, newdata = compData_holdout, type = "response") / compData_holdout %>% pull(Comp_earned_count),
                               prediction_option_2 = predict(GLM15, newdata = compData_holdout, type = "response") / compData_holdout %>% pull(Comp_earned_count))

holdout_test_data %>% head()
```

```{r}
holdout_frequency = holdout_test_data %>% pull(observed_count) %>% sum() / holdout_test_data %>% pull(exposure) %>% sum()
training_frequency = compData_train %>% pull(Comp_earned_count) %>% sum() / compData_train %>% pull(Comp_earned_count) %>% sum()
adj_factor = holdout_frequency / training_frequency
holdout_test_data = holdout_test_data %>%
  mutate(normalized_prediction_option_1 = adj_factor * prediction_option_1,
         normalized_prediction_option_2 = adj_factor * prediction_option_2)
```

```{r}
gini_10b = NormalizedWeightedGini(solution = holdout_test_data %>% pull(observed_frequency),
                       weights = holdout_test_data %>% pull(exposure),
                       submission = holdout_test_data %>% pull(normalized_prediction_option_1))

gini_16 = NormalizedWeightedGini(solution = holdout_test_data %>% pull(observed_frequency),
                       weights = holdout_test_data %>% pull(exposure),
                       submission = holdout_test_data %>% pull(normalized_prediction_option_2))
              
```

# Final model validation
Finally, our final model is GLM16 with variables: factor(Accident_year), Marital_status, vehicle_age, Annual_distance_cap_35000, num_yrs_licensed, Vehicle_wheelbase_imputed, Comp_deductible_ascat and yrs_since_purchased_imputed. All of which are still significant. Recall that it was not a clear cut whether or not to include Comp_deductible and yrs_since_purchased in our final model. We mentioned earlier that we would do further validation. Let's compare our final model GLM16 with GLM10b which is contains the same variables except Comp_deductible and yrs_since_purchased. We calculate the normalized weighted Gini coefficient on our holdout set. The following table shows our results:
```{r}
kable(data.frame(gini_10b,gini_16), col.names = c("GLM10b", "GLM16"))
```
Thus, we find although they are quite similar, GLM16 does seem to be a better fit when comparing the Gini coefficients on holdout data. Therefore, we select GLM16 as our final model!

Note that we would need to trend the frequency model’s base rate for the model to be valid in 2021 since our training data only contains information from 2018-2020. That is, we could assign 2020 as accident year to the data we want to predict and the predicted values will have to be trended to 2021 by multiplying the predicted frequencies by a trend factor, which in our case is 1.04 (from our assumptions: loss cost increases by 4% per year).
